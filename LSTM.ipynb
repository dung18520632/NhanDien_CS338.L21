{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled81.ipynb",
      "provenance": [],
      "mount_file_id": "1e651BFNOu5ABBbBi0lqhPxJ9mRYTDIg0",
      "authorship_tag": "ABX9TyMZIDAZtscUIqZDsLhezR4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dung18520632/NhanDien_CS338.L21/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISFf1yyDZFfo"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Assignment3-SentimentAnalysis-with-LSTM.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo810XFIawwY"
      },
      "source": [
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnuioVordyGK",
        "outputId": "50248393-5de4-4cee-ab72-1885fb009173"
      },
      "source": [
        "currentDir = '/content/Assignment3-SentimentAnalysis-with-LSTM'\n",
        "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
        "print('Simplified vocabulary loaded!')\n",
        "wordsList = wordsList.tolist()\n",
        "\n",
        "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
        "wordVectors = np.float32(wordVectors)\n",
        "print ('Word embedding matrix loaded!')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simplified vocabulary loaded!\n",
            "Word embedding matrix loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbr179Pbd42o",
        "outputId": "e2be3036-d72b-44ae-849f-950e1947e357"
      },
      "source": [
        "print('Size of the vocabulary: ', len(wordsList))\n",
        "print('Size of the word embedding matrix: ', wordVectors.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the vocabulary:  19899\n",
            "Size of the word embedding matrix:  (19899, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27s_wmCKd7B0",
        "outputId": "91a8791a-499a-492a-d2d2-602933366856"
      },
      "source": [
        "import tensorflow as tf\n",
        "maxSeqLength = 10   #Maximum length of sentence\n",
        "numDimensions = 300 #Dimensions for each word vector\n",
        "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
        "\n",
        "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
        "sentence = 'Món này ăn hoài không hề biết chán'\n",
        "words = sentence.split(\" \")\n",
        "for index,word in enumerate(words):\n",
        "    word_idx = wordsList.index(word.lower())\n",
        "    sentenceIndexes[index]=word_idx\n",
        "print(sentenceIndexes)\n",
        "#  Ma trận biểu diễn:\n",
        "print('Sentence representation of word vectors:')\n",
        "print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  119  8136  4884 18791 16614 12231 15951  3371     0     0]\n",
            "Sentence representation of word vectors:\n",
            "tf.Tensor(\n",
            "[[-0.1823 -0.0638  0.2376 ...  0.1462 -0.1092  0.0137]\n",
            " [ 0.027  -0.0542  0.1437 ... -0.0913  0.0114  0.0132]\n",
            " [ 0.021   0.0102  0.0096 ...  0.411  -0.2519  0.0151]\n",
            " ...\n",
            " [-0.0239 -0.0383  0.1734 ... -0.0677 -0.096   0.0045]\n",
            " [ 0.1882 -0.292   0.0072 ...  0.5919 -0.3094 -0.1228]\n",
            " [ 0.1882 -0.292   0.0072 ...  0.5919 -0.3094 -0.1228]], shape=(10, 300), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSlZs9V3kVUO",
        "outputId": "58314642-a540-479f-c599-f71eabf9992d"
      },
      "source": [
        "sentenceIndexes"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  119,  8136,  4884, 18791, 16614, 12231, 15951,  3371,     0,\n",
              "           0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DHFCE3gd9Dl"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
        "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
        "numWords = []\n",
        "for pf in positiveFiles:\n",
        "    print(pf)\n",
        "    with open(pf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)       \n",
        "print('Positive files finished')\n",
        "\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)  \n",
        "print('Negative files finished')\n",
        "\n",
        "numFiles = len(numWords)\n",
        "print('The total number of files is', numFiles)\n",
        "print('The total number of words in the files is', sum(numWords))\n",
        "print('The average number of words in the files is', sum(numWords)/len(numWords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcnuFqCveQCN"
      },
      "source": [
        "import re\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvTpx-i_exa7"
      },
      "source": [
        "maxSeqLength = 180"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "YtrqoZbheizM",
        "outputId": "984860c9-9a40-4c07-ad13-69e92ebb20a8"
      },
      "source": [
        "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
        "nFiles = 0\n",
        "# Index of Unknow word\n",
        "unk_idx = wordsList.index('UNK')\n",
        "\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
        "        nIndexes = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            # TODO 3.2: Nếu 'word' thuộc tập 'wordsList' thì gán chỉ số của 'word' vào ma trận ids\n",
        "            if word in wordsList:\n",
        "               word_idx = wordsList.index(word)\n",
        "               ids[nFiles][word_idx] = word_idx\n",
        "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
        "            nIndexes = nIndexes + 1\n",
        "            if nIndexes >= maxSeqLength:\n",
        "                break\n",
        "        nFiles = nFiles + 1 \n",
        "\n",
        "print('Positive files are indexed!')\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
        "        nIndexes = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            # ToDo 3.2: tương tự như trên. Không khác gì hết.\n",
        "            if word in wordsList:\n",
        "               word_idx = wordsList.index(word)\n",
        "               ids[nFiles][word_idx] = word_idx\n",
        "            nIndexes = nIndexes + 1\n",
        "            if nIndexes >= maxSeqLength:\n",
        "                break\n",
        "        nFiles = nFiles + 1 \n",
        "\n",
        "print('Negative files are indexed!')\n",
        "# Save ids Matrix for future uses.\n",
        "np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-075b2b46ccaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                \u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnFiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Ngược lại: gán 'unk_idx' vào ma trận ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mnIndexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnIndexes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 5596 is out of bounds for axis 0 with size 180"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iGNNsTbetUJ",
        "outputId": "bfd1c07c-8353-480d-9a59-9c00096a7bac"
      },
      "source": [
        "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
        "print('Word indexes of the first review: ', ids.shape)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word indexes of the first review:  (30000, 180)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0cg5uIJe9MT"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "def getTrainBatch():\n",
        "    labels = []\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    for i in range(batchSize):\n",
        "        if (i % 2 == 0): \n",
        "            # Pick positive samples randomly\n",
        "            num = randint(1,13999)\n",
        "            labels.append([1,0])\n",
        "        else:\n",
        "            # Pick negative samples randomly\n",
        "            num = randint(15999,29999)\n",
        "            labels.append([0,1])\n",
        "        arr[i] = ids[num-1:num]\n",
        "    return arr, labels\n",
        "\n",
        "def getTestBatch():\n",
        "    labels = []\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    for i in range(batchSize):\n",
        "        num = randint(13999,15999)\n",
        "        if (num <= 14999):\n",
        "            labels.append([1,0])\n",
        "        else:\n",
        "            labels.append([0,1])\n",
        "        arr[i] = ids[num-1:num]\n",
        "    return arr, labels"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f40y5j6e_Ma"
      },
      "source": [
        "numDimensions = 300\n",
        "batchSize = 64\n",
        "lstmUnits = 128\n",
        "nLayers = 2\n",
        "numClasses = 2\n",
        "iterations = 30000"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWG-2WzxfAgV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = ids.copy()\n",
        "Y = np.concatenate((np.ones((15000,1)),np.zeros((15000,1))))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.067)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUOt3SXDfB4O"
      },
      "source": [
        "checkpoint_path = \"/content/training_1/weights.{epoch:04d}.hdf5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "        layers.Embedding(input_dim=19899,output_dim=numDimensions,weights=[wordVectors],input_length=maxSeqLength,trainable=False),\n",
        "        layers.LSTM(lstmUnits,return_sequences=True),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.LSTM(lstmUnits),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(2,activation='softmax')]\n",
        " )\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='Adam',\n",
        "    metrics=[\"accuracy\"])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPDuK-qwfEEK"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batchSize, epochs=30000,callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-1omSbXfs-5"
      },
      "source": [
        "weight_path = '/content/training_1/weights.0053.hdf5'\n",
        "model.load_weights(weight_path)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSIhNjwTffjn"
      },
      "source": [
        "import re\n",
        "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())\n",
        "\n",
        "def Predict_Sentence(sentence,model):\n",
        "   cleaned = cleanSentences(sentence)\n",
        "   words = cleaned.split()\n",
        "   word_vec = np.zeros((1,len(words)))\n",
        "   for i,word in enumerate(words):\n",
        "      word_idx = wordsList.index(word)\n",
        "      word_vec[0][i] = word_idx\n",
        "   result = model.predict(word_vec)\n",
        "   if np.argmax(result[0])==1: print('Positive')\n",
        "   else: print('Negative')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "794DO_DKiCZy",
        "outputId": "8ee64c74-70bf-4865-af4b-5d3713051fad"
      },
      "source": [
        "Predict_Sentence('Món ăn này ngon quá đi Món ăn này ngon quá đi',model)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPKlSG2-iFE7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}